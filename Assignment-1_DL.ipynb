{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.What is the function of a summation junction of a neuron? What is threshold activation function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans--> The linear combiner or the summing junction adds all the products of the synapses and parameters.\n",
    "\n",
    "A threshold value determines whether a neuron should be activated or not activated in a binary step activation function. The activation function compares the input value to a threshold value. If the input value is greater than the threshold value, the neuron is activated\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.What is a step function? What is the difference of step function with threshold function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans--> A step function is a function like that used by the original Perceptron. The output is a certain value, A1, if the input sum is above a certain threshold and A0 if the input sum is below a certain threshold. The values used by the Perceptron were A1 = 1 and A0 = 0.\n",
    "\n",
    "Binary Step Function. Binary step function is a threshold-based activation function which means after a certain threshold neuron is activated and below the said threshold neuron is deactivated. In the above graph, the threshold is zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.Explain the McCulloch–Pitts model of neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans--> The motivation behind the McCulloh Pitt's Model is a biological neuron. A biological neuron takes an input signal from the dendrites and after processing it passes onto other connected neurons as the output if the signal is received positively, through axons and synapses.\n",
    "The McCulloch–Pitt neural network is considered to be the first neural network. The neurons are connected by directed weighted paths. McCulloch–Pitt neuron allows binary activation (1 ON or 0 OFF), i.e., it either fires with an activation 1 or does not fire with an activation of 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.Explain the ADALINE network model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans--> MADALINE is a three-layer, fully connected, feed-forward artificial neural network architecture for classification that uses ADALINE units in its hidden and output layers, i.e. its activation function is the sign function. The three-layer network uses memistors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.What is the constraint of a simple perceptron? Why it may fail with a real-world data set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans--> A perceptron model has limitations as follows:\n",
    "The output of a perceptron can only be a binary number (0 or 1) due to the hard limit transfer function. Perceptron can only be used to classify the linearly separable sets of input vectors. If input vectors are non-linear, it is not easy to classify them properly.\n",
    "Perceptrons only represent linearly separable problems. They fail to converge if the training examples are not linearly separable. This brings into picture the delta rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.What is linearly inseparable problem? What is the role of the hidden layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans-->If we have two classes: one positive and the other negative. We say they're separable if there's a classifier whose decision boundary separates the positive objects from the negative ones. If such a decision boundary is a linear function of the features, we say that the classes are linearly separable.If decision boundary is not linear function of the features we say that the classes are linearly inseparable.\n",
    "The role of the Hidden Layers is to identify features from the input data and use these to correlate between a given input and the correct output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.Explain XOR problem in case of a simple perceptron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans--> The XOR, or “exclusive or”, problem is a classic problem in ANN research. It is the problem of using a neural network to predict the outputs of XOR logic gates given two binary inputs. An XOR function should return a true value if the two inputs are not equal and a false value if they are equal.\n",
    "\n",
    "The XOR problem with neural networks can be solved by using Multi-Layer Perceptrons or a neural network architecture with an input layer, hidden layer, and output layer. So during the forward propagation through the neural networks, the weights get updated to the corresponding layers and the XOR logic gets executed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.Design a multi-layer perceptron to implement A XOR B."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans--> The XOR problem with neural networks can be solved by using Multi-Layer Perceptrons or a neural network architecture with an input layer, hidden layer, and output layer. So during the forward propagation through the neural networks, the weights get updated to the corresponding layers and the XOR logic gets executed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.Explain the single-layer feed forward architecture of ANN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans--> A layer is formed by taking a processing element and combining it with other processing elements. When a layer of the processing nodes is formed, the inputs can be connected to these nodes with various weights, resulting in a series of outputs, one per node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.Explain the competitive network architecture of ANN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans--> The architecture of neural networks is made up of an input, output, and hidden layer. Neural networks themselves, or artificial neural networks (ANNs), are a subset of machine learning designed to mimic the processing power of a human brain.\n",
    "ANN architecture in Neural Network is a part of Machine Learning and also very crucial because its structure is similar to the human brain. It also functions like a brain by sending neural signals from one end to the other.\n",
    "\n",
    "Competitive learning is a form of unsupervised learning in artificial neural networks, in which nodes compete for the right to respond to a subset of the input data. A variant of Hebbian learning, competitive learning works by increasing the specialization of each node in the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11.Consider a multi-layer feed forward neural network. Enumerate and explain steps in the backpropagation algorithm used to train the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans--> Backpropagation is a process involved in training a neural network. It involves taking the error rate of a forward propagation and feeding this loss backward through the neural network layers to fine-tune the weights. Backpropagation is the essence of neural net training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12.What are the advantages and disadvantages of neural networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans--> Advantage of neural networks -- A neural network can implement tasks that a linear program cannot. When an item of the neural network declines, it can continue without some issues by its parallel features. A neural network determines and does not require to be reprogrammed.\n",
    "\n",
    "Disadvantages include its \"black box\" nature, greater computational burden, proneness to overfitting, and the empirical nature of model development."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13.Write short notes on any two of the following:\n",
    "1.Biological neuron\n",
    "2.ReLU function\n",
    "3.Single-layer feed forward ANN\n",
    "4.Gradient descent\n",
    "5.Recurrent networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Biological neuron -- Typical biological neurons are individual cells, each composed of the main body of the cell along with many tendrils that extend from that body. Neurons, also known as nerve cells, send and receive signals from your brain. While neurons have a lot in common with other types of cells, they're structurally and functionally unique. Specialized projections called axons allow neurons to transmit electrical and chemical signals to other cells.\n",
    "\n",
    "2.ReLU function -- A rectified linear unit (ReLU) is an activation function that introduces the property of non-linearity to a deep learning model and solves the vanishing gradients issue. \"It interprets the positive part of its argument. It is one of the most popular activation functions in deep learning. ReLU helps to prevent the exponential growth in the computation required to operate the neural network. If the CNN scales in size, the computational cost of adding extra ReLUs increases.\n",
    "\n",
    "3.Single-layer feed forward ANN -- The concept is of feedforward ANN having only one weighted layer. In other words, we can say the input layer is fully connected to the output layer.A single layer neural network is a type of artificial neural network where there is only one hidden layer between the input and output layers. This is the classic architecture before the deep learning became popular. In this tutorial, you will get a chance to build a neural network with only a single hidden layer.\n",
    "\n",
    "4.Gradient descent -- Gradient descent is an optimization algorithm which is commonly-used to train machine learning models and neural networks. Gradient descent is an optimization algorithm which is commonly-used to train machine learning models and neural networks. Training data helps these models learn over time, and the cost function within gradient descent specifically acts as a barometer, gauging its accuracy with each iteration of parameter updates.\n",
    "\n",
    "5.Recurrent networks -- A recurrent neural network is a type of artificial neural network commonly used in speech recognition and natural language processing. Recurrent neural networks recognize data's sequential characteristics and use patterns to predict the next likely scenario."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
